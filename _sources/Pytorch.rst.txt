PyTorch Models
==============

Interfacing with PyTorch allows us to conveniently fit neural network potentials using descriptors
that exist in LAMMPS. We may then use these neural network models to run high-performance MD 
simulations in LAMMPS. These capabilities are explained below.

Getting Started
---------------

First we need a python environment with PyTorch and LAMMPS installed. See `LAMMPS Installation docs <Installation.html#lammps-installation>`__ 
for more details on installing LAMMPS. Here we start with a speedy minimal description LAMMPS setup::

    # Create and activate a new conda environment

    conda create --name fitsnap python=3.10
    conda activate fitsnap

    # Install dependencies with your conda's pip 

    pip install numpy torch scipy virtualenv psutil pandas tabulate mpi4py Cython

    # Clone the LAMMPS repo

    git clone https://github.com/lammps/lammps

    # Make a LAMMPS build specifically for FitSNAP

    LAMMPS_DIR=/path/to/lammps
    mkdir $LAMMPS_DIR/build-fitsnap
    cd $LAMMPS_DIR/build-fitsnap
    cmake ../cmake -DLAMMPS_EXCEPTIONS=yes \
                   -DBUILD_SHARED_LIBS=yes \
                   -DMLIAP_ENABLE_PYTHON=yes \
                   -DPKG_PYTHON=yes \
                   -DPKG_ML-SNAP=yes \
                   -DPKG_ML-IAP=yes \
                   -DPKG_ML-PACE=yes \
                   -DPKG_SPIN=yes \
                   -DPYTHON_EXECUTABLE:FILEPATH=`which python`
    make
    make install-python

To add these extra :code:`-D` flags in :code:`ccmake`, go to the Advanced Options section.

Set the following environment variables so that your Python can find LAMMPS::

    LAMMPS_DIR=/path/to/lammps
    export LD_LIBRARY_PATH=$LAMMPS_DIR/build-fitsnap:$LD_LIBRARY_PATH # Use DYLD_LIBRARY_PATH for MacOS
    export PYTHONPATH=$LAMMPS_DIR/python:$PYTHONPATH
    SITE_PACKAGES_DIR=`python -c "import site; print(site.getsitepackages()[0])"`
    export PYTHONPATH=${SITE_PACKAGES_DIR}:$PYTHONPATH # So that ML-IAP package can find torch

Alternatively to setting environment variables, symbolic links to these paths are also appropriate.

To make sure installs and paths are working, please see `LAMMPS Installation docs <Installation.html#lammps-installation>`__.

Now we are ready to fit potentials with FitSNAP, and run MD with those potentials in LAMMPS. Get 
FitSNAP with::

    cd /path/to/where/FitSNAP/will/be
    git clone https://github.com/FitSNAP/FitSNAP
    FITSNAP_DIR=/path/to/FitSNAP
    export PYTHONPATH=$FITSNAP_DIR/python:$PYTHONPATH

Fit a neural network for tantalum::

    cd $FITSNAP_DIR/examples/Ta_PyTorch_NN
    python -m fitsnap3 Ta-example.in --overwrite

Run high-performance MD with this neural network potential::

    cd MD
    mpirun -np 4 ${LAMMPS_DIR}/fitsnap-build/lmp < in.run

More details on setting up input scripts for NN fitting are given below.

Fitting Neural Network Potentials
---------------------------------

Similarly to how we fit linear models, we can input descriptors into nonlinear models such as 
neural networks. To do this, we can use the same FitSNAP input script that we use for linear 
models, with some slight changes to the sections. First we must add a :code:`PYTORCH` section, 
which for the tantalum example looks like::

    [PYTORCH]
    layer_sizes =  num_desc 60 60 1
    learning_rate = 1.5e-4 
    num_epochs = 100
    batch_size = 4
    save_state_output = Ta_Pytorch.pt
    energy_weight = 1e-2
    force_weight = 1.0
    training_fraction = 1.0
    multi_element_option = 1
    num_elements = 1

We must also add a :code:`nonlinear = 1` key in the :code:`CALCULATOR` section, and set 
:code:`solver = PYTORCH` in the :code:`SOLVER` section. Now the input script is ready to fit a 
neural network potential.

The :code:`PYTORCH` section keys are explained in more detail below.

- :code:`layer_sizes` determines the network architecture. We lead with a :code:`num_desc` parameter
  which tells FitSNAP that the number of nodes in the first layer are equal to the number of 
  descriptors. The argument here is a list where each element determines the number of nodes in 
  each layer.

- :code:`learning_rate` determines how fast the network minimizes the loss function. We find that
  a learning rate around :code:`1e-4` works well when fitting to forces, and when using our current
  loss function.

- :code:`num_epochs` sets the number of gradient descent iterations.

- :code:`batch_size` determines how many configs to average gradients for when looping over batches
  in a single epoch. We find that a batch size around 4 works well for our models.

- :code:`save_state_output` is the name of the PyTorch model file to write after every
  epoch. This model can be loaded for testing purposes later.

- :code:`save_state_input` is the name of a PyTorch model that may be loaded for the purpose of 
  restarting an existing fit, or for calculating test errors.

- :code:`energy_weight` is a scalar constant multiplied by the mean squared energy error in the 
  loss function. Declaring this parameter will override the weights in the GROUPS section for all 
  configs. We therefore call this the *global energy weight*. If you want to specify energy weights 
  for each group, do so in the GROUPS section.

- :code:`force_weight` is a scalar constant multiplied by the mean squared force error in the loss
  function. Declaring this parameter will override the weights in the GROUPS section for all 
  configs. We therefore call this the *global force weight*. If you want to specify force weights 
  for each group, do so in the GROUPS section.

- :code:`training_fraction` is a decimal fraction of how much of the total data should be trained
  on. The leftover :code:`1.0 - training_fraction` portion is used for calculating validation errors
  during a fit. Declaring this parameter will override the training/testing fractions in the GROUPS
  section for all configs. We therefore call this the *global training fraction*. If you want to 
  specify training/testing fractions for each group, do so in the GROUPS section.

- :code:`multi_element_option` is a scalar that determines how to handle multiple element types.

    - 1: All element types share the same network. Descriptors may still be different per type.
    - 2: Each element type has its own network.
    - 3: (Coming soon) One-hot encoding of element types, where each type shares the same network.

- :code:`num_elements` number of unique atom elements, or more specifically number of unique 
  networks.

- :code:`manual_seed_flag` set to 0 by default, can set to 1 if want to force a random seed which is
  useful for debugging purposes.


Outputs and Error Calculation
-----------------------------

FitSNAP outputs include files that aid in error calculation, and files that can be used to restart 
a fit or even run MD simulations in LAMMPS.

Error/Comparison files
^^^^^^^^^^^^^^^^^^^^^^

After training a potential, FitSNAP produces outputs that can be used to intrepret the quality of a 
fit on the training and/or validation data. The following comparison files are written after a fit:

- :code:`energy_comparison.dat` energy comparisons for all configs in the training set. Each row 
  corresponds to a specific configuration in the training set. The first column is the model energy, 
  and the 2nd column is the target energy. 

- :code:`energy_comparison_val.dat` energy comparisons for all configs in the validation set. 
  Format is same as above.

- :code:`force_comparison.dat` force comparisons for all atoms in all configs in the training set.
  Each row corresponds to a single atom's Cartesian component for a specific config in the training 
  set. The first column is the model energy, and the 2nd column is the target energy.

- :code:`force_comparison_val.dat` same as above, but for the validation set.

- :code:`loss_vs_epochs.dat` training and validation loss as a function of epochs, to check convergence.

These outputs allow you to compare the configuration energies, or per-atom forces, however you want
after a fit. For example, in the `Ta_PyTorch_NN example <https://github.com/FitSNAP/FitSNAP/tree/master/examples/Ta_PyTorch_NN>`_
, we provide python scripts that help post-process these files to calculate mean absolute error or 
plot comparisons in energies and forces.

PyTorch model files
^^^^^^^^^^^^^^^^^^^

FitSNAP outputs two PyTorch :code:`.pt` models file after fitting. One is used for restarting a fit
based on an existing model, specifically the model name supplied by the user in the 
:code:`save_state_output` keyword of the input script. In the `Ta_PyTorch_NN example <https://github.com/FitSNAP/FitSNAP/tree/master/examples/Ta_PyTorch_NN>`_
we can see this keyword is :code:`Ta_Pytorch.pt`. This file will therefore be saved every epoch, and 
it may be fed into FitSNAP via the :code:`save_state_input` keyword to restart another fit from that
particular model.

The other PyTorch model is used for running MD simulations in LAMMPS after a fit. This file has the 
name :code:`FitTorch_Pytorch.pt`, and is used to run MD in LAMMPS via the ML-IAP package. An example 
is given for tantalum here: https://github.com/FitSNAP/FitSNAP/tree/master/examples/Ta_PyTorch_NN/MD 

Calculate errors on a test set
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Users may want to use models to calculate errors on a test set that was completely separate from the
training/validation sets used in fitting. To do this, we change the input script to read an existing
PyTorch model file, e.g. for Ta::

    [PYTORCH]
    layer_sizes =  num_desc 60 60 1
    learning_rate = 1.5e-4 
    num_epochs = 1 ##### Set to 1 for calculating test errors
    batch_size = 4
    save_state_input = Ta_Pytorch.pt ##### Load an existing model
    energy_weight = 1e-2
    force_weight = 1.0
    training_fraction = 1.0
    multi_element_option = 1
    num_elements = 1

Notice how we are now using :code:`save_state_input` instead of :code:`save_state_output`, and that 
we set :code:`num_epochs = 1`. This will load the existing PyTorch model, and perform a single epoch
which involves calculating the energy and force comparisons (mentioned above) for the current model, 
on whatever user-defined groups of configs in the groups section.We can therefore use the energy and 
force comparison files here to calculate mean absolute errors, e.g. with the script in 
the `Ta_PyTorch_NN example <https://github.com/FitSNAP/FitSNAP/tree/master/examples/Ta_PyTorch_NN>`_

Training Performance
--------------------

As seen in the :code:`Ta_Pytorch_NN` example, fitting to ~300 configs (each with ~12 atoms) takes 
about ~0.5 s/epoch. The number of epochs required, and therefore total time of your fit, will depend 
on the size of your dataset *and* the :code:`batch_size`. For example, the :code:`Ta_Pytorch_NN` 
might take ~200 epochs to fully converge (see :code:`loss_vs_epochs.dat`). In this example, however, 
we used :code:`batch_size=4`, meaning that each epoch involved :code:`~300/4 = ~75` gradient descent 
minimizations as we cycled through batches. For much larger datasets, the network will experience 
more cycles through the batches with each epoch, and therefore may require less epochs to reach 
the same convergence.

For data sets of ~10,000 configs and ~50 atoms per config, training will take ~12-24 hours, or about 
20 minutes per epoch. 

We do not recommend using datasets much larger than this since training may take days/weeks.

Mini-batch network training is embarassingly parallel up to the batch size, but currently FitSNAP 
does not support parallelized NN training.

GPU Acceleration
^^^^^^^^^^^^^^^^

FitSNAP supports GPU acceleration via PyTorch. With small batch sizes, however, most of the benefit 
of GPU parallelization comes from evaluating the NN model and calculating gradients. You will not see 
a large benefit of GPUs using a small batch size unless you have a large NN model (e.g. > 1 million 
parameters). If you have a small model, you may see a speedup on GPUs using a large enough batch size, 
but this has not been tested.


